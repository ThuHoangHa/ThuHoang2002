{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into the train/validation/test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to split your full dataset into train/validation/test datasets, and reliably use the same datasets for your modeling tasks later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set a random seed to ensure reproducibility across runs\n",
    "RNG_SEED = 42\n",
    "np.random.seed(seed=RNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-processed dataset\n",
    "\n",
    "We will start with the processed dataset that we saved from the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full DataFrame shape: (3612, 3)\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "data_path = os.path.join(PATH, '../data_abs/cp_data_abs_cleaned.csv')\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f'Full DataFrame shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the DataFrame into your input variables ($X$) and target variables ($y$)\n",
    "\n",
    "The $X$ will be used as the input data, and $y$ will be used as the prediction targets for your ML model.\n",
    "\n",
    "If your target variables are discrete (such as `metal`/`non-metal` or types of crystal structures), then you will be performing a classification task.\n",
    "In our case, since our target variables are continuous values (absorption coefficient), we are performing a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (3612, 2)\n",
      "Shape of y: (3612,)\n"
     ]
    }
   ],
   "source": [
    "X = df[['formula', 'energy']]\n",
    "y = df['abs']\n",
    "\n",
    "print(f'Shape of X: {X.shape}')\n",
    "print(f'Shape of y: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data (and a word of caution)\n",
    "### Normally, we could simply split the data with a simple `sklearn` function\n",
    "\n",
    "The scikit-learn `train_test_split` function randomly splits a dataset into train and test datasets.\n",
    "Typically, you can use `train_test_split` to first split your data into \"train\" and \"test\" datasets, and then use the function again to split your \"train\" data into \"train\" and \"validation\" dataset splits.\n",
    "\n",
    "As a rule of thumb, you can roughly aim for the following dataset proportions when splitting your data:\n",
    "\n",
    "| | train split | validation split | test split |\n",
    "| --- | --- | --- | --- |\n",
    "| proportion<br> of original<br> dataset | 50% to 70% | 20% to 30% | 10% to 20% |\n",
    "\n",
    "If you have copious amounts of data, it may suffice to train your models on just 50% of the data; that way, you have a larger amount of data samples to validate and to test with.\n",
    "If you however have a smaller dataset and thus very few training samples for your models, you may wish to increase your proportion of training data during dataset splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.20\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mRNG_SEED)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RNG_SEED)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_rows = len(X_train)\n",
    "#print(f'There are in total {num_rows} rows in the X_train DataFrame.')\n",
    "\n",
    "#num_unique_formulae = len(X_train['formula'].unique())\n",
    "#print(f'But there are only {num_unique_formulae} unique formulae!\\n')\n",
    "\n",
    "#print('Unique formulae and their number of occurances in the X_train DataFrame:')\n",
    "#print(X_train['formula'].value_counts(), '\\n')\n",
    "#print('Unique formulae and their number of occurances in the X_test DataFrame:')\n",
    "#print(X_test['formula'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data, cautiously (manually)\n",
    "\n",
    "First we get a list of all of the unique formulae in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3612\n"
     ]
    }
   ],
   "source": [
    "unique_formulae = X['formula'].unique()\n",
    "print(f'{len(unique_formulae)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training formulae: 2529\n",
      "Number of validation formulae: 722\n",
      "Number of testing formulae: 361\n"
     ]
    }
   ],
   "source": [
    "# Set a random seed to ensure reproducibility across runs\n",
    "np.random.seed(seed=RNG_SEED)\n",
    "\n",
    "# Store a list of all unique formulae\n",
    "all_formulae = unique_formulae.copy()\n",
    "\n",
    "# Define the proportional size of the dataset split\n",
    "val_size = 0.20\n",
    "test_size = 0.10\n",
    "train_size = 1 - val_size - test_size\n",
    "\n",
    "# Calculate the number of samples in each dataset split\n",
    "num_val_samples = int(round(val_size * len(unique_formulae)))\n",
    "num_test_samples = int(round(test_size * len(unique_formulae)))\n",
    "num_train_samples = int(round((1 - val_size - test_size) * len(unique_formulae)))\n",
    "\n",
    "# Randomly choose the formulate for the validation dataset, and remove those from the unique formulae list\n",
    "val_formulae = np.random.choice(all_formulae, size=num_val_samples, replace=False)\n",
    "all_formulae = [f for f in all_formulae if f not in val_formulae]\n",
    "\n",
    "# Randomly choose the formulate for the test dataset, and remove those from the unique formulae list\n",
    "test_formulae = np.random.choice(all_formulae, size=num_test_samples, replace=False)\n",
    "all_formulae = [f for f in all_formulae if f not in test_formulae]\n",
    "\n",
    "# The remaining formulae will be used for the training dataset\n",
    "train_formulae = all_formulae.copy()\n",
    "\n",
    "print('Number of training formulae:', len(train_formulae))\n",
    "print('Number of validation formulae:', len(val_formulae))\n",
    "print('Number of testing formulae:', len(test_formulae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape: (2529, 3)\n",
      "validation dataset shape: (722, 3)\n",
      "test dataset shape: (361, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the original dataset into the train/validation/test datasets using the formulae lists above\n",
    "df_train = df[df['formula'].isin(train_formulae)]\n",
    "df_val = df[df['formula'].isin(val_formulae)]\n",
    "df_test = df[df['formula'].isin(test_formulae)]\n",
    "\n",
    "print(f'train dataset shape: {df_train.shape}')\n",
    "print(f'validation dataset shape: {df_val.shape}')\n",
    "print(f'test dataset shape: {df_test.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be sure that we really only have mutually exclusive formulae within each of the datasets, we can do the following to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of common formulae in intersection 1: 0; common formulae: set()\n",
      "# of common formulae in intersection 2: 0; common formulae: set()\n",
      "# of common formulae in intersection 3: 0; common formulae: set()\n"
     ]
    }
   ],
   "source": [
    "train_formulae = set(df_train['formula'].unique())\n",
    "val_formulae = set(df_val['formula'].unique())\n",
    "test_formulae = set(df_test['formula'].unique())\n",
    "\n",
    "common_formulae1 = train_formulae.intersection(test_formulae)\n",
    "common_formulae2 = train_formulae.intersection(val_formulae)\n",
    "common_formulae3 = test_formulae.intersection(val_formulae)\n",
    "\n",
    "print(f'# of common formulae in intersection 1: {len(common_formulae1)}; common formulae: {common_formulae1}')\n",
    "print(f'# of common formulae in intersection 2: {len(common_formulae2)}; common formulae: {common_formulae2}')\n",
    "print(f'# of common formulae in intersection 3: {len(common_formulae3)}; common formulae: {common_formulae3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save split datasets to csv\n",
    "\n",
    "Finally, after splitting the dataset into train/validation/test dataset splits, you can save them to disk for you to use later.\n",
    "\n",
    "By saving these dataset splits into files, you can then later reproducibly use these same exact splits during your subsequent model training and comparison steps.\n",
    "Use the same datasets for all your models---that way, you can ensure a fair comparison.\n",
    "\n",
    "Also, when you publish your results, you can include these dataset splits, so that others can use the exact datasets in their own studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving these splits into csv files\n",
    "PATH = os.getcwd()\n",
    "\n",
    "train_path = os.path.join(PATH, '../data_abs/cp_train.csv')\n",
    "val_path = os.path.join(PATH, '../data_abs/cp_val.csv')\n",
    "test_path = os.path.join(PATH, '../data_abs/cp_test.csv')\n",
    "\n",
    "df_train.to_csv(train_path, index=False)\n",
    "df_val.to_csv(val_path, index=False)\n",
    "df_test.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, keep the test dataset locked away and forget about it until you have finalized your model!\n",
    "**Never look at the test dataset!!** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
